<base href="https://www.cs.utexas.edu/users/novak/asg-lex.html">
<HTML>
<!--  asg-lex.html          G. Novak           10 Aug 17    -->
<!--    -->
<HEAD>
  <TITLE>CS 375: Lexical Analyzer using Lex</TITLE>
</HEAD>
<BODY>

<H2>CS 375: Lexical Analyzer using Lex</H2>

<H3>Due: September 28, 2017.</H3>
<P>
Redo the lexical analyzer for Pascal,
with the same specifications as in the previous assignment, but
using the Lex system to generate the syntactic analysis program.
<P>
Lex is described in the textbook in Section 3.5 .  There is a <tt>man</tt>
description of <tt>lex</tt> on-line; also see the file <tt>commentsinlex.txt</tt> for
useful hints on handling comments in Lex.  Several books on Lex and Yacc
are available, although you should not need them for this assignment.
<a href="lexpaper.htm">
<tt>http://www.cs.utexas.edu/users/novak/lexpaper.htm </tt> </A>
is a paper on <tt>lex</tt>.
<a href="http://flex.sourceforge.net/">
<tt>http://flex.sourceforge.net/ </tt></a>
has the manual on Flex (free software version of Lex).
<P>
Lex allows the tokens of a language to be described using regular expressions.
The Lex compiler compiles these to produce finite automaton tables for
an automaton to parse the language.  When a complete token has been
found, user-specified actions (written in C) are executed to
produce the desired output.  Lex always produces two outputs:
<OL>
<LI> <I>What is it?</I>: the kind of token that was found.  This is the
<tt>return()</tt> value, a small integer, e.g. <tt>return(NUMBER)</tt>.
<LI> The <I>value</I> of the token that was found.  This is returned as a
side-effect by setting the variable <tt>yylval</tt> to the value;
for our purposes, <tt>yylval</tt> will always be of type <tt>TOKEN</tt>.
</OL>
<P>
Lex allows a lexical analyzer to be constructed more easily than
by writing one as a program.  For this assignment, it <b>is</b> allowable
to use C library routines such as <tt>sscanf</tt> to help with number
conversion.
<P>
Several files are provided to help you get started.  The file <tt>lexasu.l</tt>
is an implementation of a simple Lex scanner similar to that shown in
Figure 3.18 in the book.  You can try this program with the following
commands and data to Unix:
<P>
<TABLE>
<TR><TD> <tt>  make lexasu        </tt>
   </TD><TD>   Use lex to compile <tt>lexasu.l --> lexasu.c --> lexasu </tt></TD></TR>
<TR><TD> <tt>  lexasu                      </tt>
   </TD><TD>   Execute compiled <tt>lexasu</tt></TD></TR>
<TR><TD> <tt>  if switch then 3.14 else 4 </tt>
   </TD><TD>   Test data</TD></TR>
<TR><TD> <tt>  ^D                         </tt>
   </TD><TD>   Control-D for end of file to stop.</TD></TR>
</TABLE>
<P>
The file <tt>lex2.l</tt> creates tokens similar to the ones
in the first assignment; you can try it as follows:
<P>
<TABLE>
<TR><TD> <tt>  make lex2        </tt>
   </TD><TD>   Use lex to compile <tt>lex2.l --> lex2.c --> lex2 </tt></TD></TR>
<TR><TD> <tt>  lex2                      </tt>
   </TD><TD>   Execute compiled <tt>lex2</tt></TD></TR>
<TR><TD> <tt>  if x &gt; y then 3.14 else 4.5 </tt>
   </TD><TD>   Test data</TD></TR>
<TR><TD> <tt>  ^D                         </tt>
   </TD><TD>   Control-D for end of file to stop.</TD></TR>
</TABLE>
<P>
 You can copy <tt>lex2.l</tt> to <tt>lexan.l</tt>
(<tt>cp lex2.l lexan.l</tt>) and use <tt>lexan.l</tt> as the
starting point for your program; <tt>lexan.l</tt> can be run using
<tt>make lexer </tt> as described in comments in the file.
<P>
<H3>Testing:</H3>
<P>
Test your program on the files <tt>graph1.pas</tt>
and <tt>scantst.pas </tt> .  Error checking for numbers out of range
is <b>not</b> required for this assignment.  How well does the lex
program work on the difficult cases of <tt>scantst.pas</tt>?  Is it
as robust as your hand-written lexical analyzer?

</BODY>

</HTML>
