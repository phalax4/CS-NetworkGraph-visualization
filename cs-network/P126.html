<base href="http://www.cs.utexas.edu/~mooney/ir-course//proj2//">
<html>

<head>
<title>CS 371R Information Retrieval and Web Search: Project 2</title>
</head>

<BODY BGCOLOR="#FFFFFF" TEXT="#000000" style="max-width:800px; margin: 20 auto;" >
<center>
<h2>Project 2<br> CS 371R: Information Retrieval and Web Search <br> Evaluating
the Performance of Relevance Rated Feedback </h2> <br> Due: October 12, 2017

</center> <br> <h3> Existing Framework for Evaluating Retrieval </h3>

As discussed in class, a basic system for evaluating vector-space retrieval
(VSR) is available in <code>/u/mooney/ir-code/ir/eval/</code>. See the <a
href="../doc/ir/eval/package-summary.html">Javadoc </a> for this system.  Use
the <a href="../doc/ir/eval/Experiment.html#main(java.lang.String[])"> main
method </a> for <a href="../doc/ir/eval/Experiment.html"> Experiment</a> to
index a set of documents, then process queries, evaluate the results compared
to known relevant documents, and finally generate a recall-precision curve
using the interpolation method discussed in class.

<p> You can use the documents in the Cystic-Fibrosis (CF) corpus (<code>/u/mooney/ir-code/corpora/cf/</code>) as a set of test documents.  This corpus
contains 1,239 "documents" (actually just medical article title and abstracts).
A set of 100 queries with the correct documents determined to be relevant to
these queries is in 
<code>/u/mooney/ir-code/queries/cf/queries</code>.

<p> As discussed in class, Experiment can be used to produce recall-precision
curves for this document/query corpus. Here is a <a href="trace.txt"> trace </a> of
running such an experiment.  The program also generates as output a <code>".gplot"</code>
file that <a href="http://www.duke.edu/~hpgavin/gnuplot.html">gnuplot</a> can
use to generate a recall-precision graph (plot), such as <a href="rp.pdf">this graph</a>.
To create a pdf plot file execute the following command:<br/><br/>
<code>gnuplot filename.gplot | ps2pdf - filename.pdf</code>.<br/><br/>

The <code>gnuplot</code> command creates a postscript (*.ps) file and this output
is directly piped into the <code>ps2pdf</code> command (Note the "-") which then produces
a pdf <code>filename.pdf</code>.
<p>
A set of sample results files that I generated for the CF data are in <code>
/u/mooney/ir-code/results/cf/</code>.
<p>
You can also edit the ".gplot" files yourself to create graphs combining the
results of multiple runs of Experiment (such as with this <a
href="cf-rp-all.gplot"> ".gplot" file</a> and resulting <a
href="cf-rp-all.pdf">pdf plot file</a>) in order to compare different methods.
<p>
The existing Experiment assumes simple binary gold-standard relevance
judgements.  However, real-valued gold-standard ratings of relevance are more
informative, and our CF data actually comes with ratings on a 3-level scale
(0:not relevant, 1:marginally relevant, 2:very relevant) from 4 judges. In
order to produce a single relevance rating, I averaged the scores of the 4
judges and scaled the result to produce a real-valued rating between 0 and 1.
The rated query file in <code>/u/mooney/ir-code/queries/cf/queries-rated</code>
has the results. For each query, each relevant document is followed by a 0-1
relevance rating.
<p>
The variant of Experiment in ExperimentRated evaluates on rated queries such
as <code>queries-rated</code>.  In addition to producing recall-precision
curves (which should be the same as those from Experiment), it also produces
NDCG results that utilize the continuous relevance ratings.  Average
NDCG values for all ranks 1-10 are printed at the end of the run and also
written to a file with an <code>".ndcg"</code> extension. In addition, a *.ndcg.gplot
file is created that allows for the creation of NDCG plot similar to the Precision-Recall
plot (<a href="experimentRatedExampleNDCG.pdf">an example NDCG plot</a>).
Here is a <a href="trace-rated.txt"> trace</a> of
running an ExperimentRated experiment. 
<p>
<em> NOTE: The Java code and class file for <code>ExperimentRated</code> has
    been added to the code under <code>/u/mooney/ir-code/</code>.  Thus, if you
    downloaded a local copy of the code, you need to download the new <code>ExperimentRated</code>
    *.java and *.class files.  If you used the network code directly, you shouldn't have
    to change anything.
</em>
</p>

<h3>Your Task</h3>

<h4> Relevance-Rated Feedback </h4>
<p>
Code for performing binary relevance feedback is included in the VSR system.  It is
invoked by using the "-feedback" flag, in which case, after viewing a retrieved
document, the user is asked to rate it as either relevant or irrelevant to the
query.  Then, by using the "r" (redo) command, this feedback will be used to
revise the query vector (using the Ide_Regular method), which is then used to
produce a new set of retrievals. You can see a <a
href="feedback-trace.txt">trace</a> of using relevance feedback in VSR.
<p>
Part of your task is to modify the existing code for relevance feedback to
accept continuous real-valued feedback rather than just binary feedback.
Create a new version of the Feedback class called FeedbackRated that allows
continuous relevance rating. The new versions of the <code>addGood</code>
and <code>addBad</code> methods for this class should also be given a
real-valued rating of how good or bad the given document is.  In order to
modify the Ide Regular algorithm implemented in Feedback to handle real-values
ratings, just multiply a document vector by its corresponding rating value
before adding or subtracting it from the query. Implement a specialization of
InvertedIndex called InvertedIndexRated that uses continuous relevance-rated
feedback.  Allow the user to provide real-valued ratings between -1 (very
irrelevant) and +1 (very relevant).  You can see
a <a href="relevance-rated-trace.txt">trace</a> of using my version of
relevance-rated feedback.

<h4> Evaluating Relevance-Rated Feedback </h4>
<p>
An important question that can be addressed experimentally is: Does relevance
feedback improve retrieval results?  As discussed in class and the text, when
evaluating relevance feedback, one must be sure <b>not</b> to include in the
final evaluated results, results on documents for which feedback has been
explicitly provided (in machine learning, this error is called "testing on the
training data").  Results must be evaluated <i>only</i> for the documents for
which no feedback has been provided.
<p>
Your assignment is to produce a specialization (subclass) of the
ExperimentRated object (ExperimentRelFeedbackRated) that supports fair
evaluation of rated relevance feedback, and then use this code to produce
recall-precision curves and NDCG results that evaluate the effect of different
types of relevance feedback on retrieval performance on the CF corpus.
<p>
The main method for your new experiment class should accept an additional (4th)
command-line argument, the number of documents, N, for which to simulate
feedback (in addition to the existing inputs (corpus directory, query file, and
output file) and option flags accepted by ExperimentRated).  Then after each
test query, the system should use the information on the correct relevant
documents from the query file to simulate user relevance feedback for the top N
documents in the set of initial ranked retrievals.  Then it should use this
feedback to revise and re-execute the query to produce a new set of ranked
retrievals.  These final retrieval results should then be evaluated, but first
all documents for which feedback has been provided must be removed from the
retrieval array and the list of correct retrievals.  The final output should be
a recall-precision graph and NDCG results for the reduced ("residual") test corpus.
<p>
You should compare your rated-relevance feedback, to normal "binary" relevance
feedback, and to no feedback.  For rated-relevance feedback, since only
relevant document ratings are included in <code>queries-rated</code>, you
should use a consistent rating of (negative) 1 for all irrelevant documents
(i.e. those that do not appear in the list of gold-standard relevant documents
for that query), and the gold-standard 0-1 positive rating for relevant
documents listed for that query in <code>queries-rated</code>.  To produce
binary feedback, ExperimentRelFeedbackRated should accept a flag
"<code>-binary</code>" that reduces all ratings to positive or negative 1.  To
compare to no feedback as a control condition, ExperimentRelFeedbackRated
should accept a flag "<code>-control</code>" that prevents any revision of the
query and simply evaluates the original results on the "residual" corpus (the
corpus after removing the top N ranked retrievals). Example commands:
<pre>
java ir.eval.ExperimentRelFeedbackRated [Optional FLAG] [CORPORA] [QUERIES] [OUTPUT_FILENAME] [NUM_SIMULATED_FEEDBACK]
java ir.eval.ExperimentRelFeedbackRated -control /u/mooney/ir-code/corpora/cf/ /u/mooney/ir-code/queries/cf/queries-rated control 3
java ir.eval.ExperimentRelFeedbackRated -binary /u/mooney/ir-code/corpora/cf/ /u/mooney/ir-code/queries/cf/queries-rated binary 3
java ir.eval.ExperimentRelFeedbackRated /u/mooney/ir-code/corpora/cf/ /u/mooney/ir-code/queries/cf/queries-rated rated 3
</pre>

<p>
Produce recall-precision curves and NDCG plots for retrieval ranks 1 to 10
for these three approaches.  For each approach, try  N= 1, 3 and 5, i.e.
try providing simulated relevance feedback for the top 1, 3, and 5 initial
ranked retrieval results. Note that you cannot fairly compare results using
different values of N since the residual test corpus is different for different
values of N. Therefore, the control condition is different for different values
of N and must use the appropriate residual corpus for that value of N.
Therefore, you should produce three recall-precision curves and three NDCG plots,
one for each value of N (1,3,5), comparing the three approaches
using the same residual test corpus (i.e. There should be 6 plots in total: 
Prec-Recall plots for N=1,3,5 and NDCG plots for N=1,3,5.
Each graph should have 3 curves for rated-feedback, binary-feedback, and no-feedback.).
<p>
Your report should summarize your approach, present the results in well
organized graphs, and answer at least the following questions (<em>You
    should explicitly answer these questions, i.e. in your report put "Q1. Does..?" 
    and then give the answer underneath so that we do not have to search for your answers.</em>):
<ol>
<li>Does using feedback improve retrieval accuracy? Why or why not?
<li>Does rated-relevance feedback improve retrieval accuracy over binary
relevance feedback? Why or why not?
<li> Do the recall-precision curves and the NDCG results show different results
regarding the relative performance of these three different approaches?  Why or
  why not?
</ol>

<h3>Submission</h3>
<p>
In submitting your solution, follow the general course instructions on
<a href="../proj-submit-info.html">submitting projects</a> on the course homepage.  Along with that, follow these specific instructions for Project 2:

<ul>
<li> Create at least the following new classes described above:
    <ol>
        <li>FeedbackRated which extends Feedback</li>
        <li>InvertedIndexRated which extends InvertedIndex</li>
        <li>ExperimentRelFeedbackRated which extends ExperimentRated</li>
    </ol>

    <li> For this assignment, you need to submit the following files:</li>
<ol>
    <li> <code>[PREFIX]_code.zip</code> - Your code in zip file (*.java and *.class file). Please do not modify the original java files but extend each class and override the appropriate methods.</li>
    <li> <code>[PREFIX]_report.pdf</code> - A PDF report of your experiment as described above with the 6 plots referenced in the instructions.</li>
    <li> <code>[PREFIX]_rated_trace.txt</code> - Trace file of a test of InvertedIndexRated with the <strong><em>same commands</em></strong> as in this <a href="relevance-rated-trace.txt">trace</a>. </li>
    <li> <code>[PREFIX]_exp_trace.txt</code> - Trace file of a test of
  ExperimentRelFeedbackRated similar to this <a href="trace-rated.txt">ExperimentRated
  trace</a> but also containing, for each query, information on the Feedback utilized.  This Feedback information should be given as in the following example output:
  <pre>
Query 6: What is the effect of water or other therapeutic agents on the 
physical properties (viscosity, elasticity) of sputum or bronchial 
secretions from CF patients?
Returned 955 documents.
24 truly relevant documents.
Feedback:
Positive docs: [RN-00593, RN-00031, RN-00441]
Negative docs: [RN-00047, RN-00976]
Executing New Expanded and Reweighted Query: 
   1 is relevant; Recall =  4.762%; Precision =  100.0%
   ...
</pre>
 </li>
</ol>
The files listed under "Turned In" on Canvas should be:<br/><br/>
    <code>
        proj1_jd1234_code.zip</br>
        proj1_jd1234_rated_trace.txt</br>
        proj1_jd1234_exp_trace.txt</br>
        proj1_jd1234_report.pdf</br>
    </code>
    <br/>
    and the zip file should have at least the following contents:<br/>
            <code style="white-space:pre;">
$ unzip -l proj1_jd1234_code.zip 
Archive:  proj1_jd1234_code.zip
  Length      Date    Time    Name
---------  ---------- -----   ----
    21067  2015-09-14 12:57   ir/vsr/InvertedIndexRated.java
    10049  2015-09-14 17:26   ir/vsr/InvertedIndexRated.class
    21067  2015-09-14 12:57   ir/vsr/FeedbackRated.java
    10049  2015-09-14 17:26   ir/vsr/FeedbackRated.class
    21067  2015-09-14 12:57   ir/eval/ExperimentRelFeedbackRated.java
    10049  2015-09-14 17:26   ir/eval/ExperimentRelFeedbackRated.class
---------                     -------
    91106                     6 files
            </code>

</ul>
<h3>Grading Criteria</h3>
<ul>
<li> 10%: Your program compiles successfully and functions normally without throwing out any exception.</li>
<li> 10%: Your implementation is efficient. For example, it shouldn't change the overall time complexity. Also, it shouldn't significantly increase the average time it takes to respond to a query.</li>
<li>35% Working code the correctly implements rated feedback and experimental evaluation of feedback</li>
<li>10%: Good programming style, with necessary comments, intuitive variable/function names and appropriate indent.</li>
<li>35%: Quality of report, clear presentation of results and good analysis & discussion answering all of the questions.</li>
</ul>
</body>
</html>

